---
references:
- id: abukausarWebCrawlerReview2013
  abstract: >-
    Information Retrieval deals with searching and retrieving information within
    the documents and it also searches the online databases and internet. Web
    crawler is defined as a program or software which traverses the Web and
    downloads web documents in a methodical, automated manner. Based on the type
    of knowledge, web crawler is usually divided in three types of crawling
    techniques: General Purpose Crawling, Focused crawling and Distributed
    Crawling. In this paper, the applicability of Web Crawler in the field of
    web search and a review on Web Crawler to different problem domains in web
    search is discussed.
  accessed:
    - year: 2022
      month: 5
      day: 5
  author:
    - family: AbuKausar
      given: Md.
    - family: S. Dhaka
      given: V.
    - family: Kumar Singh
      given: Sanjeev
  citation-key: abukausarWebCrawlerReview2013
  container-title: International Journal of Computer Applications
  container-title-short: IJCA
  DOI: 10.5120/10440-5125
  ISSN: '09758887'
  issue: '2'
  issued:
    - year: 2013
      month: 2
      day: 15
  language: en
  page: 31-36
  source: DOI.org (Crossref)
  title: 'Web Crawler: A Review'
  title-short: Web Crawler
  type: article-journal
  URL: http://research.ijcaonline.org/volume63/number2/pxc3885125.pdf
  volume: '63'

- id: batsakisImprovingPerformanceFocused2009
  abstract: >-
    This work addresses issues related to the design and implementation of
    focused crawlers. Several variants of state-of-the-art crawlers relying on
    web page content and link information for estimating the relevance of web
    pages to a given topic are proposed. Particular emphasis is given to
    crawlers capable of learning not only the content of relevant pages (as
    classic crawlers do) but also paths leading to relevant pages. A novel
    learning crawler inspired by a previously proposed Hidden Markov Model (HMM)
    crawler is described as well. The crawlers have been implemented using the
    same baseline implementation (only the priority assignment function differs
    in each crawler) providing an unbiased evaluation framework for a
    comparative analysis of their performance. All crawlers achieve their
    maximum performance when a combination of web page content and (link) anchor
    text is used for assigning download priorities to web pages. Furthermore,
    the new HMM crawler improved the performance of the original HMM crawler and
    also outperforms classic focused crawlers in searching for specialized
    topics.
  accessed:
    - year: 2022
      month: 5
      day: 6
  author:
    - family: Batsakis
      given: Sotiris
    - family: Petrakis
      given: Euripides G.M.
    - family: Milios
      given: Evangelos
  citation-key: batsakisImprovingPerformanceFocused2009
  container-title: Data & Knowledge Engineering
  container-title-short: Data & Knowledge Engineering
  DOI: 10.1016/j.datak.2009.04.002
  ISSN: 0169023X
  issue: '10'
  issued:
    - year: 2009
      month: 10
  language: en
  page: 1001-1013
  source: DOI.org (Crossref)
  title: Improving the performance of focused web crawlers
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0169023X0900055X
  volume: '68'

- id: boldiUbiCrawlerScalableFully2004
  abstract: >-
    We report our experience in implementing UbiCrawler, a scalable distributed
    Web crawler, using the Java programming language. The main features of
    UbiCrawler are platform independence, linear scalability, graceful
    degradation in the presence of faults, a very effective assignment function
    (based on consistent hashing) for partitioning the domain to crawl, and more
    in general the complete decentralization of every task. The necessity of
    handling very large sets of data has highlighted some limitations of the
    Java APIs, which prompted the authors to partially reimplement them.
    Copyright c 2004 John Wiley & Sons, Ltd.
  accessed:
    - year: 2022
      month: 5
      day: 6
  author:
    - family: Boldi
      given: Paolo
    - family: Codenotti
      given: Bruno
    - family: Santini
      given: Massimo
    - family: Vigna
      given: Sebastiano
  citation-key: boldiUbiCrawlerScalableFully2004
  container-title: 'Software: Practice and Experience'
  container-title-short: 'Softw: Pract. Exper.'
  DOI: 10.1002/spe.587
  ISSN: 0038-0644, 1097-024X
  issue: '8'
  issued:
    - year: 2004
      month: 7
      day: 10
  language: en
  page: 711-726
  source: DOI.org (Crossref)
  title: 'UbiCrawler: a scalable fully distributed Web crawler'
  title-short: UbiCrawler
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/10.1002/spe.587
  volume: '34'

- id: BRIN1998107
  abstract: >-
    In this paper, we present Google, a prototype of a large-scale search engine
    which makes heavy use of the structure present in hypertext. Google is
    designed to crawl and index the Web efficiently and produce much more
    satisfying search results than existing systems. The prototype with a full
    text and hyperlink database of at least 24 million pages is available at
    http://google.stanford.edu/ To engineer a search engine is a challenging
    task. Search engines index tens to hundreds of millions of Web pages
    involving a comparable number of distinct terms. They answer tens of
    millions of queries every day. Despite the importance of large-scale search
    engines on the Web, very little academic research has been done on them.
    Furthermore, due to rapid advance in technology and Web proliferation,
    creating a Web search engine today is very different from three years ago.
    This paper provides an in-depth description of our large-scale Web search
    engine — the first such detailed public description we know of to date.
    Apart from the problems of scaling traditional search techniques to data of
    this magnitude, there are new technical challenges involved with using the
    additional information present in hypertext to produce better search
    results. This paper addresses this question of how to build a practical
    large-scale system which can exploit the additional information present in
    hypertext. Also we look at the problem of how to effectively deal with
    uncontrolled hypertext collections where anyone can publish anything they
    want.
  author:
    - family: Brin
      given: Sergey
    - family: Page
      given: Lawrence
  citation-key: BRIN1998107
  container-title: Computer Networks and ISDN Systems
  DOI: https://doi.org/10.1016/S0169-7552(98)00110-X
  ISSN: 0169-7552
  issue: '1'
  issued:
    - year: 1998
  page: 107-117
  title: The anatomy of a large-scale hypertextual Web search engine
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S016975529800110X
  volume: '30'

- id: CAPTwelveYears
  abstract: >-
    The CAP theorem asserts that any networked shared-data system can have only
    two of three desirable properties (Consistency, Availability and Partition
    Tolerance). In this IEEE article, author Eric Brewer discusses how designers
    can optimize consistency and availability by explicitly handling partitions,
    thereby achieving some trade-off of all three.
  accessed:
    - year: 2022
      month: 5
      day: 17
  citation-key: CAPTwelveYears
  container-title: InfoQ
  language: en
  title: 'CAP Twelve Years Later: How the "Rules" Have Changed'
  title-short: CAP Twelve Years Later
  type: webpage
  URL: >-
    https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed/

- id: cattellScalableSQLNoSQL2011
  abstract: >-
    In this paper, we examine a number of SQL and socalled “NoSQL” data stores
    designed to scale simple OLTP-style application loads over many servers.
    Originally motivated by Web 2.0 applications, these systems are designed to
    scale to thousands or millions of users doing updates as well as reads, in
    contrast to traditional DBMSs and data warehouses. We contrast the new
    systems on their data model, consistency mechanisms, storage mechanisms,
    durability guarantees, availability, query support, and other dimensions.
    These systems typically sacrifice some of these dimensions, e.g.
    database-wide transaction consistency, in order to achieve others, e.g.
    higher availability and scalability.
  accessed:
    - year: 2022
      month: 5
      day: 17
  author:
    - family: Cattell
      given: Rick
  citation-key: cattellScalableSQLNoSQL2011
  container-title: ACM SIGMOD Record
  container-title-short: SIGMOD Rec.
  DOI: 10.1145/1978915.1978919
  ISSN: 0163-5808
  issue: '4'
  issued:
    - year: 2011
      month: 5
      day: 6
  language: en
  page: 12-27
  source: DOI.org (Crossref)
  title: Scalable SQL and NoSQL data stores
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1978915.1978919
  volume: '39'

- id: changBigtableDistributedStorage2008
  abstract: >-
    Bigtable is a distributed storage system for managing structured data that
    is designed to scale to a very large size: petabytes of data across
    thousands of commodity servers. Many projects at Google store data in
    Bigtable, including web indexing, Google Earth, and Google Finance. These
    applications place very different demands on Bigtable, both in terms of data
    size (from URLs to web pages to satellite imagery) and latency requirements
    (from backend bulk processing to real-time data serving). Despite these
    varied demands, Bigtable has successfully provided a ﬂexible,
    high-performance solution for all of these Google products. In this paper we
    describe the simple data model provided by Bigtable, which gives clients
    dynamic control over data layout and format, and we describe the design and
    implementation of Bigtable.
  accessed:
    - year: 2022
      month: 5
      day: 17
  author:
    - family: Chang
      given: Fay
    - family: Dean
      given: Jeffrey
    - family: Ghemawat
      given: Sanjay
    - family: Hsieh
      given: Wilson C.
    - family: Wallach
      given: Deborah A.
    - family: Burrows
      given: Mike
    - family: Chandra
      given: Tushar
    - family: Fikes
      given: Andrew
    - family: Gruber
      given: Robert E.
  citation-key: changBigtableDistributedStorage2008
  container-title: ACM Transactions on Computer Systems
  container-title-short: ACM Trans. Comput. Syst.
  DOI: 10.1145/1365815.1365816
  ISSN: 0734-2071, 1557-7333
  issue: '2'
  issued:
    - year: 2008
      month: 6
  language: en
  page: 1-26
  source: DOI.org (Crossref)
  title: 'Bigtable: A Distributed Storage System for Structured Data'
  title-short: Bigtable
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1365815.1365816
  volume: '26'

- id: choEvolutionWebImplications
  abstract: >-
    In this paper we study how to build an eﬀective incremental crawler. The
    crawler selectively and incrementally updates its index and/or local
    collection of web pages, instead of periodically refreshing the collection
    in batch mode. The incremental crawler can improve the “freshness” of the
    collection signiﬁcantly and bring in new pages in a more timely manner. We
    ﬁrst present results from an experiment conducted on more than half million
    web pages over 4 months, to estimate how web pages evolve over time. Based
    on these experimental results, we compare various design choices for an
    incremental crawler and discuss their trade-oﬀs. We propose an architecture
    for the incremental crawler, which combines the best design choices.
  author:
    - family: Cho
      given: Junghoo
    - family: Garcia-Molina
      given: Hector
  citation-key: choEvolutionWebImplications
  language: en
  page: '18'
  source: Zotero
  title: The Evolution of the Web and Implications for an Incremental Crawler
  type: article-journal

- id: grolingerDataManagementCloud2013
  abstract: >-
    Advances in Web technology and the proliferation of mobile devices and
    sensors connected to the Internet have resulted in immense processing and
    storage requirements. Cloud computing has emerged as a paradigm that
    promises to meet these requirements. This work focuses on the storage aspect
    of cloud computing, specifically on data management in cloud environments.
    Traditional relational databases were designed in a different hardware and
    software era and are facing challenges in meeting the performance and scale
    requirements of Big Data. NoSQL and NewSQL data stores present themselves as
    alternatives that can handle huge volume of data. Because of the large
    number and diversity of existing NoSQL and NewSQL solutions, it is difficult
    to comprehend the domain and even more challenging to choose an appropriate
    solution for a specific task. Therefore, this paper reviews NoSQL and NewSQL
    solutions with the objective of: (1) providing a perspective in the field,
    (2) providing guidance to practitioners and researchers to choose the
    appropriate data store, and (3) identifying challenges and opportunities in
    the field. Specifically, the most prominent solutions are compared focusing
    on data models, querying, scaling, and security related capabilities.
    Features driving the ability to scale read requests and write requests, or
    scaling data storage are investigated, in particular partitioning,
    replication, consistency, and concurrency control. Furthermore, use cases
    and scenarios in which NoSQL and NewSQL data stores have been used are
    discussed and the suitability of various solutions for different sets of
    applications is examined. Consequently, this study has identified challenges
    in the field, including the immense diversity and inconsistency of
    terminologies, limited documentation, sparse comparison and benchmarking
    criteria, and nonexistence of standardized query languages.
  accessed:
    - year: 2022
      month: 5
      day: 17
  author:
    - family: Grolinger
      given: Katarina
    - family: Higashino
      given: Wilson A
    - family: Tiwari
      given: Abhinav
    - family: Capretz
      given: Miriam AM
  citation-key: grolingerDataManagementCloud2013
  container-title: 'Journal of Cloud Computing: Advances, Systems and Applications'
  container-title-short: J Cloud Comp
  DOI: 10.1186/2192-113X-2-22
  ISSN: 2192-113X
  issue: '1'
  issued:
    - year: 2013
      month: 12
  language: en
  page: '22'
  source: DOI.org (Crossref)
  title: 'Data management in cloud environments: NoSQL and NewSQL data stores'
  title-short: Data management in cloud environments
  type: article-journal
  URL: >-
    https://journalofcloudcomputing.springeropen.com/articles/10.1186/2192-113X-2-22
  volume: '2'

- id: meltonSQLLanguageSummary1996
  accessed:
    - year: 2022
      month: 5
      day: 14
  author:
    - family: Melton
      given: Jim
  citation-key: meltonSQLLanguageSummary1996
  container-title: ACM Computing Surveys
  container-title-short: ACM Comput. Surv.
  DOI: 10.1145/234313.234374
  ISSN: 0360-0300, 1557-7341
  issue: '1'
  issued:
    - year: 1996
      month: 3
  language: en
  page: 141-143
  source: DOI.org (Crossref)
  title: SQL language summary
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/234313.234374
  volume: '28'

- id: ParallelCrawlers
  accessed:
    - year: 2022
      month: 5
      day: 5
  citation-key: ParallelCrawlers
  title: Parallel Crawlers
  type: webpage
  URL: https://ra.ethz.ch/CDstore/www2002/refereed/108/index.html

- id: >-
    wirelesscommunicationandcomputingstudentcsedepartmentg.h.raisoniinstituteofengineeringandtechnologyforwomennagpurindiaStudyWebCrawler2014
  abstract: >-
    Due to the current size of the Web and its dynamic nature, building an
    efficient search mechanism is very important. A vast number of web pages are
    continually being added every day, and information is constantly changing.
    Search engines are used to extract valuable Information from the internet.
    Web crawlers are the principal part of search engine, is a computer program
    or software that browses the World Wide Web in a methodical, automated
    manner or in an orderly fashion. It is an essential method for collecting
    data on, and keeping in touch with the rapidly increasing Internet. This
    Paper briefly reviews the concepts of web crawler, its architecture and its
    various types.
  accessed:
    - year: 2022
      month: 5
      day: 5
  author:
    - literal: >-
        Wireless Communication and Computing) student, CSE Department, G.H.
        Raisoni Institute of Engineering and Technology for Women, Nagpur, India
    - family: Udapure
      given: Trupti V.
    - family: Kale
      given: Ravindra D.
    - family: Dharmik
      given: Rajesh C.
  citation-key: >-
    wirelesscommunicationandcomputingstudentcsedepartmentg.h.raisoniinstituteofengineeringandtechnologyforwomennagpurindiaStudyWebCrawler2014
  container-title: IOSR Journal of Computer Engineering
  container-title-short: IOSRJCE
  DOI: 10.9790/0661-16160105
  ISSN: 22788727, 22780661
  issue: '1'
  issued:
    - year: 2014
  language: en
  page: 01-05
  source: DOI.org (Crossref)
  title: Study of Web Crawler and its Different Types
  type: article-journal
  URL: >-
    http://www.iosrjournals.org/iosr-jce/papers/Vol16-issue1/Version-6/A016160105.pdf
  volume: '16'

- id: zobelInvertedFilesText2006
  abstract: >-
    The technology underlying text search engines has advanced dramatically in
    the past decade. The development of a family of new index representations
    has led to a wide range of innovations in index storage, index construction,
    and query evaluation. While some of these developments have been
    consolidated in textbooks, many specific techniques are not widely known or
    the textbook descriptions are out of date. In this tutorial, we introduce
    the key techniques in the area, describing both a core implementation and
    how the core can be enhanced through a range of extensions. We conclude with
    a comprehensive bibliography of text indexing literature.
  accessed:
    - year: 2022
      month: 5
      day: 18
  author:
    - family: Zobel
      given: Justin
    - family: Moffat
      given: Alistair
  citation-key: zobelInvertedFilesText2006
  container-title: ACM Computing Surveys
  container-title-short: ACM Comput. Surv.
  DOI: 10.1145/1132956.1132959
  ISSN: 0360-0300, 1557-7341
  issue: '2'
  issued:
    - year: 2006
      month: 7
      day: 25
  language: en
  page: '6'
  source: DOI.org (Crossref)
  title: Inverted files for text search engines
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1132956.1132959
  volume: '38'
...
