---
references:
- id: abukausarWebCrawlerReview2013
  abstract: >-
    Information Retrieval deals with searching and retrieving information within
    the documents and it also searches the online databases and internet. Web
    crawler is defined as a program or software which traverses the Web and
    downloads web documents in a methodical, automated manner. Based on the type
    of knowledge, web crawler is usually divided in three types of crawling
    techniques: General Purpose Crawling, Focused crawling and Distributed
    Crawling. In this paper, the applicability of Web Crawler in the field of
    web search and a review on Web Crawler to different problem domains in web
    search is discussed.
  accessed:
    - year: 2022
      month: 5
      day: 5
  author:
    - family: AbuKausar
      given: Md.
    - family: S. Dhaka
      given: V.
    - family: Kumar Singh
      given: Sanjeev
  citation-key: abukausarWebCrawlerReview2013
  container-title: International Journal of Computer Applications
  container-title-short: IJCA
  DOI: 10.5120/10440-5125
  ISSN: '09758887'
  issue: '2'
  issued:
    - year: 2013
      month: 2
      day: 15
  language: en
  page: 31-36
  source: DOI.org (Crossref)
  title: 'Web Crawler: A Review'
  title-short: Web Crawler
  type: article-journal
  URL: http://research.ijcaonline.org/volume63/number2/pxc3885125.pdf
  volume: '63'

- id: balakrishnanStemmingLemmatizationComparison2014
  abstract: >-
    The current study proposes to compare document retrieval precision
    performances based on language modeling techniques, particularly stemming
    and lemmatization. Stemming is a procedure to reduce all words with the same
    stem to a common form whereas lemmatization removes inflectional endings and
    returns the base or dictionary form of a word. Comparisons were also made
    between these two techniques with a baseline ranking algorithm (i.e. with no
    language processing). A search engine was developed and the algorithms were
    tested based on a test collection. Both mean average precisions and
    histograms indicate stemming and lemmatization to outperform the baseline
    algorithm. As for the language modeling techniques, lemmatization produced
    better precision compared to stemming, however the differences are
    insignificant. Overall the findings suggest that language modeling
    techniques improves document retrieval, with lemmatization technique
    producing the best result.
  accessed:
    - year: 2022
      month: 5
      day: 20
  author:
    - family: Balakrishnan
      given: Vimala
    - family: Ethel
      given: Lloyd-Yemoh
  citation-key: balakrishnanStemmingLemmatizationComparison2014
  container-title: Lecture Notes on Software Engineering
  container-title-short: LNSE
  DOI: 10.7763/LNSE.2014.V2.134
  ISSN: '23013559'
  issue: '3'
  issued:
    - year: 2014
  language: en
  page: 262-267
  source: DOI.org (Crossref)
  title: 'Stemming and Lemmatization: A Comparison of Retrieval Performances'
  title-short: Stemming and Lemmatization
  type: article-journal
  URL: http://www.lnse.org/show-34-165-1.html
  volume: '2'

- id: batsakisImprovingPerformanceFocused2009
  abstract: >-
    This work addresses issues related to the design and implementation of
    focused crawlers. Several variants of state-of-the-art crawlers relying on
    web page content and link information for estimating the relevance of web
    pages to a given topic are proposed. Particular emphasis is given to
    crawlers capable of learning not only the content of relevant pages (as
    classic crawlers do) but also paths leading to relevant pages. A novel
    learning crawler inspired by a previously proposed Hidden Markov Model (HMM)
    crawler is described as well. The crawlers have been implemented using the
    same baseline implementation (only the priority assignment function differs
    in each crawler) providing an unbiased evaluation framework for a
    comparative analysis of their performance. All crawlers achieve their
    maximum performance when a combination of web page content and (link) anchor
    text is used for assigning download priorities to web pages. Furthermore,
    the new HMM crawler improved the performance of the original HMM crawler and
    also outperforms classic focused crawlers in searching for specialized
    topics.
  accessed:
    - year: 2022
      month: 5
      day: 6
  author:
    - family: Batsakis
      given: Sotiris
    - family: Petrakis
      given: Euripides G.M.
    - family: Milios
      given: Evangelos
  citation-key: batsakisImprovingPerformanceFocused2009
  container-title: Data & Knowledge Engineering
  container-title-short: Data & Knowledge Engineering
  DOI: 10.1016/j.datak.2009.04.002
  ISSN: 0169023X
  issue: '10'
  issued:
    - year: 2009
      month: 10
  language: en
  page: 1001-1013
  source: DOI.org (Crossref)
  title: Improving the performance of focused web crawlers
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0169023X0900055X
  volume: '68'

- id: boldiUbiCrawlerScalableFully2004
  abstract: >-
    We report our experience in implementing UbiCrawler, a scalable distributed
    Web crawler, using the Java programming language. The main features of
    UbiCrawler are platform independence, linear scalability, graceful
    degradation in the presence of faults, a very effective assignment function
    (based on consistent hashing) for partitioning the domain to crawl, and more
    in general the complete decentralization of every task. The necessity of
    handling very large sets of data has highlighted some limitations of the
    Java APIs, which prompted the authors to partially reimplement them.
    Copyright c 2004 John Wiley & Sons, Ltd.
  accessed:
    - year: 2022
      month: 5
      day: 6
  author:
    - family: Boldi
      given: Paolo
    - family: Codenotti
      given: Bruno
    - family: Santini
      given: Massimo
    - family: Vigna
      given: Sebastiano
  citation-key: boldiUbiCrawlerScalableFully2004
  container-title: 'Software: Practice and Experience'
  container-title-short: 'Softw: Pract. Exper.'
  DOI: 10.1002/spe.587
  ISSN: 0038-0644, 1097-024X
  issue: '8'
  issued:
    - year: 2004
      month: 7
      day: 10
  language: en
  page: 711-726
  source: DOI.org (Crossref)
  title: 'UbiCrawler: a scalable fully distributed Web crawler'
  title-short: UbiCrawler
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/10.1002/spe.587
  volume: '34'

- id: BRIN1998107
  abstract: >-
    In this paper, we present Google, a prototype of a large-scale search engine
    which makes heavy use of the structure present in hypertext. Google is
    designed to crawl and index the Web efficiently and produce much more
    satisfying search results than existing systems. The prototype with a full
    text and hyperlink database of at least 24 million pages is available at
    http://google.stanford.edu/ To engineer a search engine is a challenging
    task. Search engines index tens to hundreds of millions of Web pages
    involving a comparable number of distinct terms. They answer tens of
    millions of queries every day. Despite the importance of large-scale search
    engines on the Web, very little academic research has been done on them.
    Furthermore, due to rapid advance in technology and Web proliferation,
    creating a Web search engine today is very different from three years ago.
    This paper provides an in-depth description of our large-scale Web search
    engine — the first such detailed public description we know of to date.
    Apart from the problems of scaling traditional search techniques to data of
    this magnitude, there are new technical challenges involved with using the
    additional information present in hypertext to produce better search
    results. This paper addresses this question of how to build a practical
    large-scale system which can exploit the additional information present in
    hypertext. Also we look at the problem of how to effectively deal with
    uncontrolled hypertext collections where anyone can publish anything they
    want.
  author:
    - family: Brin
      given: Sergey
    - family: Page
      given: Lawrence
  citation-key: BRIN1998107
  container-title: Computer Networks and ISDN Systems
  DOI: https://doi.org/10.1016/S0169-7552(98)00110-X
  ISSN: 0169-7552
  issue: '1'
  issued:
    - year: 1998
  page: 107-117
  title: The anatomy of a large-scale hypertextual Web search engine
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S016975529800110X
  volume: '30'

- id: CAPTwelveYears
  abstract: >-
    The CAP theorem asserts that any networked shared-data system can have only
    two of three desirable properties (Consistency, Availability and Partition
    Tolerance). In this IEEE article, author Eric Brewer discusses how designers
    can optimize consistency and availability by explicitly handling partitions,
    thereby achieving some trade-off of all three.
  accessed:
    - year: 2022
      month: 5
      day: 17
  citation-key: CAPTwelveYears
  container-title: InfoQ
  language: en
  title: 'CAP Twelve Years Later: How the "Rules" Have Changed'
  title-short: CAP Twelve Years Later
  type: webpage
  URL: >-
    https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed/

- id: cattellScalableSQLNoSQL2011
  abstract: >-
    In this paper, we examine a number of SQL and socalled “NoSQL” data stores
    designed to scale simple OLTP-style application loads over many servers.
    Originally motivated by Web 2.0 applications, these systems are designed to
    scale to thousands or millions of users doing updates as well as reads, in
    contrast to traditional DBMSs and data warehouses. We contrast the new
    systems on their data model, consistency mechanisms, storage mechanisms,
    durability guarantees, availability, query support, and other dimensions.
    These systems typically sacrifice some of these dimensions, e.g.
    database-wide transaction consistency, in order to achieve others, e.g.
    higher availability and scalability.
  accessed:
    - year: 2022
      month: 5
      day: 17
  author:
    - family: Cattell
      given: Rick
  citation-key: cattellScalableSQLNoSQL2011
  container-title: ACM SIGMOD Record
  container-title-short: SIGMOD Rec.
  DOI: 10.1145/1978915.1978919
  ISSN: 0163-5808
  issue: '4'
  issued:
    - year: 2011
      month: 5
      day: 6
  language: en
  page: 12-27
  source: DOI.org (Crossref)
  title: Scalable SQL and NoSQL data stores
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1978915.1978919
  volume: '39'

- id: changBigtableDistributedStorage2008
  abstract: >-
    Bigtable is a distributed storage system for managing structured data that
    is designed to scale to a very large size: petabytes of data across
    thousands of commodity servers. Many projects at Google store data in
    Bigtable, including web indexing, Google Earth, and Google Finance. These
    applications place very different demands on Bigtable, both in terms of data
    size (from URLs to web pages to satellite imagery) and latency requirements
    (from backend bulk processing to real-time data serving). Despite these
    varied demands, Bigtable has successfully provided a ﬂexible,
    high-performance solution for all of these Google products. In this paper we
    describe the simple data model provided by Bigtable, which gives clients
    dynamic control over data layout and format, and we describe the design and
    implementation of Bigtable.
  accessed:
    - year: 2022
      month: 5
      day: 17
  author:
    - family: Chang
      given: Fay
    - family: Dean
      given: Jeffrey
    - family: Ghemawat
      given: Sanjay
    - family: Hsieh
      given: Wilson C.
    - family: Wallach
      given: Deborah A.
    - family: Burrows
      given: Mike
    - family: Chandra
      given: Tushar
    - family: Fikes
      given: Andrew
    - family: Gruber
      given: Robert E.
  citation-key: changBigtableDistributedStorage2008
  container-title: ACM Transactions on Computer Systems
  container-title-short: ACM Trans. Comput. Syst.
  DOI: 10.1145/1365815.1365816
  ISSN: 0734-2071, 1557-7333
  issue: '2'
  issued:
    - year: 2008
      month: 6
  language: en
  page: 1-26
  source: DOI.org (Crossref)
  title: 'Bigtable: A Distributed Storage System for Structured Data'
  title-short: Bigtable
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1365815.1365816
  volume: '26'

- id: choEvolutionWebImplications
  abstract: >-
    In this paper we study how to build an eﬀective incremental crawler. The
    crawler selectively and incrementally updates its index and/or local
    collection of web pages, instead of periodically refreshing the collection
    in batch mode. The incremental crawler can improve the “freshness” of the
    collection signiﬁcantly and bring in new pages in a more timely manner. We
    ﬁrst present results from an experiment conducted on more than half million
    web pages over 4 months, to estimate how web pages evolve over time. Based
    on these experimental results, we compare various design choices for an
    incremental crawler and discuss their trade-oﬀs. We propose an architecture
    for the incremental crawler, which combines the best design choices.
  author:
    - family: Cho
      given: Junghoo
    - family: Garcia-Molina
      given: Hector
  citation-key: choEvolutionWebImplications
  language: en
  page: '18'
  source: Zotero
  title: The Evolution of the Web and Implications for an Incremental Crawler
  type: article-journal

- id: farberWhichKnowledgeGraph2018
  abstract: >-
    In recent years, DBpedia, Freebase, OpenCyc, Wikidata, and YAGO have been
    published as noteworthy large, cross-domain, and freely available knowledge
    graphs. Although extensively in use, these knowledge graphs are hard to
    compare against each other in a given setting. Thus, it is a challenge for
    researchers and developers to pick the best knowledge graph for their
    individual needs. In our recent survey, we devised and applied data quality
    criteria to the above-mentioned knowledge graphs. Furthermore, we proposed a
    framework for finding the most suitable knowledge graph for a given setting.
    With this paper we intend to ease the access to our in-depth survey by
    presenting simplified rules that map individual data quality requirements to
    specific knowledge graphs. However, this paper does not intend to replace
    our previously introduced decision-support framework. For an informed
    decision on which KG is best for you we still refer to our in-depth survey.
  accessed:
    - year: 2022
      month: 5
      day: 21
  author:
    - family: Färber
      given: Michael
    - family: Rettinger
      given: Achim
  citation-key: farberWhichKnowledgeGraph2018
  issued:
    - year: 2018
      month: 9
      day: 28
  number: arXiv:1809.11099
  publisher: arXiv
  source: arXiv.org
  title: Which Knowledge Graph Is Best for Me?
  type: article
  URL: http://arxiv.org/abs/1809.11099

- id: grolingerDataManagementCloud2013
  abstract: >-
    Advances in Web technology and the proliferation of mobile devices and
    sensors connected to the Internet have resulted in immense processing and
    storage requirements. Cloud computing has emerged as a paradigm that
    promises to meet these requirements. This work focuses on the storage aspect
    of cloud computing, specifically on data management in cloud environments.
    Traditional relational databases were designed in a different hardware and
    software era and are facing challenges in meeting the performance and scale
    requirements of Big Data. NoSQL and NewSQL data stores present themselves as
    alternatives that can handle huge volume of data. Because of the large
    number and diversity of existing NoSQL and NewSQL solutions, it is difficult
    to comprehend the domain and even more challenging to choose an appropriate
    solution for a specific task. Therefore, this paper reviews NoSQL and NewSQL
    solutions with the objective of: (1) providing a perspective in the field,
    (2) providing guidance to practitioners and researchers to choose the
    appropriate data store, and (3) identifying challenges and opportunities in
    the field. Specifically, the most prominent solutions are compared focusing
    on data models, querying, scaling, and security related capabilities.
    Features driving the ability to scale read requests and write requests, or
    scaling data storage are investigated, in particular partitioning,
    replication, consistency, and concurrency control. Furthermore, use cases
    and scenarios in which NoSQL and NewSQL data stores have been used are
    discussed and the suitability of various solutions for different sets of
    applications is examined. Consequently, this study has identified challenges
    in the field, including the immense diversity and inconsistency of
    terminologies, limited documentation, sparse comparison and benchmarking
    criteria, and nonexistence of standardized query languages.
  accessed:
    - year: 2022
      month: 5
      day: 17
  author:
    - family: Grolinger
      given: Katarina
    - family: Higashino
      given: Wilson A
    - family: Tiwari
      given: Abhinav
    - family: Capretz
      given: Miriam AM
  citation-key: grolingerDataManagementCloud2013
  container-title: 'Journal of Cloud Computing: Advances, Systems and Applications'
  container-title-short: J Cloud Comp
  DOI: 10.1186/2192-113X-2-22
  ISSN: 2192-113X
  issue: '1'
  issued:
    - year: 2013
      month: 12
  language: en
  page: '22'
  source: DOI.org (Crossref)
  title: 'Data management in cloud environments: NoSQL and NewSQL data stores'
  title-short: Data management in cloud environments
  type: article-journal
  URL: >-
    https://journalofcloudcomputing.springeropen.com/articles/10.1186/2192-113X-2-22
  volume: '2'

- id: hoganKnowledgeGraphs2022
  abstract: >-
    In this paper we provide a comprehensive introduction to knowledge graphs,
    which have recently garnered significant attention from both industry and
    academia in scenarios that require exploiting diverse, dynamic, large-scale
    collections of data. After some opening remarks, we motivate and contrast
    various graph-based data models and query languages that are used for
    knowledge graphs. We discuss the roles of schema, identity, and context in
    knowledge graphs. We explain how knowledge can be represented and extracted
    using a combination of deductive and inductive techniques. We summarise
    methods for the creation, enrichment, quality assessment, refinement, and
    publication of knowledge graphs. We provide an overview of prominent open
    knowledge graphs and enterprise knowledge graphs, their applications, and
    how they use the aforementioned techniques. We conclude with high-level
    future research directions for knowledge graphs.
  accessed:
    - year: 2022
      month: 5
      day: 20
  author:
    - family: Hogan
      given: Aidan
    - family: Blomqvist
      given: Eva
    - family: Cochez
      given: Michael
    - family: Amato
      given: Claudia
      non-dropping-particle: d'
    - family: Melo
      given: Gerard
      non-dropping-particle: de
    - family: Gutierrez
      given: Claudio
    - family: Gayo
      given: José Emilio Labra
    - family: Kirrane
      given: Sabrina
    - family: Neumaier
      given: Sebastian
    - family: Polleres
      given: Axel
    - family: Navigli
      given: Roberto
    - family: Ngomo
      given: Axel-Cyrille Ngonga
    - family: Rashid
      given: Sabbir M.
    - family: Rula
      given: Anisa
    - family: Schmelzeisen
      given: Lukas
    - family: Sequeda
      given: Juan
    - family: Staab
      given: Steffen
    - family: Zimmermann
      given: Antoine
  citation-key: hoganKnowledgeGraphs2022
  container-title: ACM Computing Surveys
  container-title-short: ACM Comput. Surv.
  DOI: 10.1145/3447772
  ISSN: 0360-0300, 1557-7341
  issue: '4'
  issued:
    - year: 2022
      month: 5
      day: 31
  page: 1-37
  source: arXiv.org
  title: Knowledge Graphs
  type: article-journal
  URL: http://arxiv.org/abs/2003.02320
  volume: '54'

- id: jivaniComparativeStudyStemming2011
  abstract: >-
    Stemming is a pre-processing step in Text Mining applications as well as a
    very common requirement of Natural Language processing functions. In fact it
    is very important in most of the Information Retrieval systems. The main
    purpose of stemming is to reduce different grammatical forms / word forms of
    a word like its noun, adjective, verb, adverb etc. to its root form. We can
    say that the goal of stemming is to reduce inflectional forms and sometimes
    derivationally related forms of a word to a common base form. In this paper
    we have discussed different methods of stemming and their comparisons in
    terms of usage, advantages as well as limitations. The basic difference
    between stemming and lemmatization is also discussed.
  author:
    - family: Jivani
      given: Anjali Ganesh
  citation-key: jivaniComparativeStudyStemming2011
  issued:
    - year: 2011
  language: en
  page: '9'
  source: Zotero
  title: A Comparative Study of Stemming Algorithms
  type: article-journal
  volume: '2'

- id: meltonSQLLanguageSummary1996
  accessed:
    - year: 2022
      month: 5
      day: 14
  author:
    - family: Melton
      given: Jim
  citation-key: meltonSQLLanguageSummary1996
  container-title: ACM Computing Surveys
  container-title-short: ACM Comput. Surv.
  DOI: 10.1145/234313.234374
  ISSN: 0360-0300, 1557-7341
  issue: '1'
  issued:
    - year: 1996
      month: 3
  language: en
  page: 141-143
  source: DOI.org (Crossref)
  title: SQL language summary
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/234313.234374
  volume: '28'

- id: ParallelCrawlers
  accessed:
    - year: 2022
      month: 5
      day: 5
  citation-key: ParallelCrawlers
  title: Parallel Crawlers
  type: webpage
  URL: https://ra.ethz.ch/CDstore/www2002/refereed/108/index.html

- id: ramosUsingTFIDFDetermine
  abstract: >-
    In this paper, we examine the results of applying Term Frequency Inverse
    Document Frequency (TF-IDF) to determine what words in a corpus of documents
    might be more favorable to use in a query. As the term implies, TF-IDF
    calculates values for each word in a document through an inverse proportion
    of the frequency of the word in a particular document to the percentage of
    documents the word appears in. Words with high TF-IDF numbers imply a strong
    relationship with the document they appear in, suggesting that if that word
    were to appear in a query, the document could be of interest to the user. We
    provide evidence that this simple algorithm efficiently categorizes relevant
    words that can enhance query retrieval. 1.
  author:
    - family: Ramos
      given: Juan
  citation-key: ramosUsingTFIDFDetermine
  source: CiteSeer
  title: Using TF-IDF to Determine Word Relevance in Document Queries
  type: document

- id: saltonInformationProcessingManagement1988
  author:
    - family: Salton
      given: Gerard
    - family: Buckley
      given: Christopher
  citation-key: saltonInformationProcessingManagement1988
  container-title: Term-weighting approaches in automatic text retrieval
  issue: '5'
  issued:
    - year: 1988
  page: 513--523
  title: Information processing \& management
  type: article-journal
  volume: '24'

- id: SnowballLanguageStemming
  accessed:
    - year: 2022
      month: 5
      day: 20
  citation-key: SnowballLanguageStemming
  title: 'Snowball: A language for stemming algorithms'
  type: webpage
  URL: http://snowball.tartarus.org/texts/introduction.html

- id: SQLiteFTS5Extension
  accessed:
    - year: 2022
      month: 5
      day: 22
  citation-key: SQLiteFTS5Extension
  title: SQLite FTS5 Extension
  type: webpage
  URL: https://www.sqlite.org/fts5.html

- id: Wikidata
  accessed:
    - year: 2022
      month: 5
      day: 21
  citation-key: Wikidata
  title: Wikidata
  type: webpage
  URL: https://www.wikidata.org/wiki/Wikidata:Main_Page

- id: willettPorterStemmingAlgorithm2006
  abstract: >-
    General review Purpose: In 1980, Porter presented a simple algorithm for
    stemming English language words. This paper summarises the main features of
    the algorithm, and highlights its role not just in modern information
    retrieval research, but also in a range of related subject domains.
  accessed:
    - year: 2022
      month: 5
      day: 20
  author:
    - family: Willett
      given: Peter
  citation-key: willettPorterStemmingAlgorithm2006
  container-title: Program
  container-title-short: Program
  DOI: 10.1108/00330330610681295
  ISSN: 0033-0337, 0033-0337
  issue: '3'
  issued:
    - year: 2006
      month: 7
  language: en
  page: 219-223
  source: DOI.org (Crossref)
  title: 'The Porter stemming algorithm: then and now'
  title-short: The Porter stemming algorithm
  type: article-journal
  URL: >-
    https://www.emerald.com/insight/content/doi/10.1108/00330330610681295/full/html
  volume: '40'

- id: >-
    wirelesscommunicationandcomputingstudentcsedepartmentg.h.raisoniinstituteofengineeringandtechnologyforwomennagpurindiaStudyWebCrawler2014
  abstract: >-
    Due to the current size of the Web and its dynamic nature, building an
    efficient search mechanism is very important. A vast number of web pages are
    continually being added every day, and information is constantly changing.
    Search engines are used to extract valuable Information from the internet.
    Web crawlers are the principal part of search engine, is a computer program
    or software that browses the World Wide Web in a methodical, automated
    manner or in an orderly fashion. It is an essential method for collecting
    data on, and keeping in touch with the rapidly increasing Internet. This
    Paper briefly reviews the concepts of web crawler, its architecture and its
    various types.
  accessed:
    - year: 2022
      month: 5
      day: 5
  author:
    - literal: >-
        Wireless Communication and Computing) student, CSE Department, G.H.
        Raisoni Institute of Engineering and Technology for Women, Nagpur, India
    - family: Udapure
      given: Trupti V.
    - family: Kale
      given: Ravindra D.
    - family: Dharmik
      given: Rajesh C.
  citation-key: >-
    wirelesscommunicationandcomputingstudentcsedepartmentg.h.raisoniinstituteofengineeringandtechnologyforwomennagpurindiaStudyWebCrawler2014
  container-title: IOSR Journal of Computer Engineering
  container-title-short: IOSRJCE
  DOI: 10.9790/0661-16160105
  ISSN: 22788727, 22780661
  issue: '1'
  issued:
    - year: 2014
  language: en
  page: 01-05
  source: DOI.org (Crossref)
  title: Study of Web Crawler and its Different Types
  type: article-journal
  URL: >-
    http://www.iosrjournals.org/iosr-jce/papers/Vol16-issue1/Version-6/A016160105.pdf
  volume: '16'

- id: YouReReady
  accessed:
    - year: 2022
      month: 5
      day: 21
  citation-key: YouReReady
  language: en-US
  title: So you’re ready to get started. – Common Crawl
  type: post-weblog
  URL: https://commoncrawl.org/the-data/get-started/

- id: zobelInvertedFilesText2006
  abstract: >-
    The technology underlying text search engines has advanced dramatically in
    the past decade. The development of a family of new index representations
    has led to a wide range of innovations in index storage, index construction,
    and query evaluation. While some of these developments have been
    consolidated in textbooks, many specific techniques are not widely known or
    the textbook descriptions are out of date. In this tutorial, we introduce
    the key techniques in the area, describing both a core implementation and
    how the core can be enhanced through a range of extensions. We conclude with
    a comprehensive bibliography of text indexing literature.
  accessed:
    - year: 2022
      month: 5
      day: 18
  author:
    - family: Zobel
      given: Justin
    - family: Moffat
      given: Alistair
  citation-key: zobelInvertedFilesText2006
  container-title: ACM Computing Surveys
  container-title-short: ACM Comput. Surv.
  DOI: 10.1145/1132956.1132959
  ISSN: 0360-0300, 1557-7341
  issue: '2'
  issued:
    - year: 2006
      month: 7
      day: 25
  language: en
  page: '6'
  source: DOI.org (Crossref)
  title: Inverted files for text search engines
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1132956.1132959
  volume: '38'
...
