# Способы хранения данных
Как было сказано в введении базы данных бывают разных типов. В нашей конкретной задаче было достаточно обычной реляционной базы данных. Так как объемы информации не настолько большие чтобы выделять под это отдельный сервер и частота запросов невелика, было решено остановиться на базе данных Sqlite.

Загрузка информации происходит в несколько этапов:

1. Собираем список нужных новостных изданий.
1. Запись списка URL в базу данных в таблицу с именем queue колонку url.
1. Дальше в несколько процессов запускается скрипт загрузчика.
1. Загрузчик берет каждую запись из очереди.
1. При помощи API CommonCrawl получает ссылки на архивы для скачивания.
1. Пакетами, которые позволяет хранящий ресурс, данные загружаются в таблицу content.
1. Текст отправляется в колонку raw_cont, а адрес страницы в колонку url

Структура таблиц:
```SQL 
TABLE man(id primary key, lastname, firstname, middlename, expert);
TABLE queue(
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    url TEXT,
    load_started DEFAULT NULL,
    load_complete DEFAULT NULL
);
TABLE content(
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    url TEXT,
    cont TEXT
);
```

Всего было выгружено 5Гб очищенных данных. Загрузка производилась в несколько процессов. Данные очищались на этапе получения от различных HTML тегов при помощи python библиотеки BeautifulSoup и проекта readability.


