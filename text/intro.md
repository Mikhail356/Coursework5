# Введение 

Введение — общее описание проблемной области информационного поиска и поиска имен в частности и формулировка задачи.

В наше время интернет является огромным хранилищем различных данных. Не все они одинаково полезны для конкретных задач. В данной работе рассматривается задача поиска экспертов в новостных публикациях, с целью нахождения связности между ними. Связность при этом может быть любого вида. Здесь мы считаем что связь это наличие совместного упоминания в публикации двух и более экспертов. Но чтобы этого достичь нужно научиться правильно искать нужные данные. Поисковые системы реализуют механизм получения срезов данных. Первым этапом в любом поиске является поверхностный сбор информации. Его осуществляют при помощи поисковых пауков [@abukausarWebCrawlerReview2013]. 

Поисковый паук это программа, которая автоматически обходит определенный заранее список адресов (URL) и заносит полученные страницы в специальную коллекцию. Из полученных страниц, в свою очередь, извлекаются новые URL и добавляются в конец исходного набора. Так они работали изначально, пока весь объем данных в интернете был сравнительно мал. Пауки различаются по типам [@wirelesscommunicationandcomputingstudentcsedepartmentg.h.raisoniinstituteofengineeringandtechnologyforwomennagpurindiaStudyWebCrawler2014]. В частности, существуют _периодические_ пауки общего назначения [@choEvolutionWebImplications]. Они скачивают все указанные в списке URL, пока не наберут требуемое автору паука количество скачанных страниц и останавливаются. Эта процедура повторяется периодически, когда возникает необходимость в обновлении данных. Они не самые эффективные с точки зрения скорости исполнения, зато отсутствует возможность не обновить заранее заданные страницы. Также существуют _пошаговые_ пауки. Они имеют постоянный размер коллекции и продолжают свою работу непрерывно. Их задача беспрерывно работать и заменять наименее "полезные" страницы на более "полезные" по некоторому правилу [@choEvolutionWebImplications]. Такой вид пауков появился в ответ на то, что данные в интернете постоянно меняются. Одни страницы появляются, другие наоборот исчезают. Не все они одинаково "полезные" и в силу ограниченности доступной памяти некоторые удаляются. Построение универсальной метрики полезности является отдельной сложной задачей. Если считать страницу важной по непрерывному количеству посещений ее, то можно удалить важную страницу на которой раз в месяц оплачиваются счета за важные услуги (например коммунальные). Или же считать полезными только те страницы на которые больше всего ежемесячная аудитория, что делает невозможным использование такого паука с некоторой узкой целью (поиска информации по своей узкой специальности). Следующим типом являются _распределенные_ пауки[@boldiUbiCrawlerScalableFully2004]. Они состоят из многих пауков общего назначения, которые проверяют URL только в определенной области интернета, частое использование которой характерно для ограниченной географически территории. Например для определенной страны характерно использование сайтов в основном на ее языке. При этом есть центральный сервер, контролирующий и распределяющий URL между ними. Таким образом, достигается большая отказоустойчивость всей системы, хоть и существует некоторое ограничение скорости в силу использования пауков общего назначения. Эту проблему призваны были решать _параллельные_ пауки [@ParallelCrawlers]. В отличие от _распределенных_ они обрабатывают единый массив URL, которые разделены на несколько машин, обрабатывающих эти URL параллельно, а не последовательно. Такое улучшение позволило повысить скорость выгрузки страниц. Подтипом пауков общего назначения являются _фокусированные_. Они обходят и добавляют в список дальнейшего обхода только те URL, по которым находится документ соответствующий некоторой теме (допустим теме поискового запроса). При этом используются различные методы подсчета обратных ссылок. Добавление новых ссылок происходит до тех пор пока не наберется нужное количество страниц или не обойдется весь список URL. В Google Inc. в 1998 году для подсчета обратных ссылок использовали PageRank [@BRIN1998107]. 

Для описания алгоритма PageRank зададим следующие условия. Пусть на странице А цитируются страницы $S_1, ..., S_n$ (присутствуют их URL), $d \in (0,1)$ параметр затухания (в работе брали $0.85$), $C(A)$ - общее количество цитируемых страниц на странице А. Тогда $PR(A) = (1-d) + d*(\frac{PR(S_1)}{C(S_1)}+\frac{PR(S_2)}{C(S_2)}+...+\frac{PR(S_n)}{C(S_n)})$, где $PR$ это PageRank. Существует множество вариантов, выбора параметров для построения фокусированных пауков. Поэтому они, в свою очередь, подразделяются на различные виды в зависимости от способа определения соответствия страницы теме и способа обработки страниц [@batsakisImprovingPerformanceFocused2009].

Когда мы научились правильно искать данные, возникает следующая задача - надежное хранение и быстрый доступ к ним. Отвечая на такой запрос появился язык SQL[@meltonSQLLanguageSummary1996] для работы с реляционными базами данных. Реляционная модель представляет собой набор двумерных таблиц. Каждая таблица состоит из строк - записей и столбцов - полей. Поля обязаны быть одного из допустимых типов. Таблицы могут быть связаны друг с другом при помощи различных ключей (ссылок). Изначально реляционные базы данных создавались для хранения на одной машине сравнительного небольшого объема данных. Когда объем их достаточно вырос старая парадигма баз данных перестала работать. CAP теорема [@CAPTwelveYears] утверждает, что любая система общих данных может обладать наибольшее двумя свойствами из следующих: согласованность - существует только одна актуальная версия данных, доступность - данные доступны в любой момент времени, стабильность - устойчивость к физическим нарушениям связности частей данных. В связи с этим стали появляться нереляционные базы данных и системы для управления ими. Они специализированы под определенные задачи и поэтому могут делать незначительные для их цели допущения в CAP теореме. В наше время можно выделить следующие типы нереляционных моделей данных[@grolingerDataManagementCloud2013]. Наиболее известной является модель _ключ-значение_. Принцип ее работы похож на идею хеш-таблицы. У каждой записи есть уникальный ключ, которому соответствует единственный хранящий запись сервер. Следующий тип это _документный_. Его идея состоит в том, что документы устроены гораздо сложнее, чем просто текстовые поля. Они могут содержать ссылки на другие документы, которые в свою очередь ссылаются на дополнительные и так далее. _Столбцовая_ модель отличается от предыдущих тем, что хранит данны в виде столбцов, а не записей. Основополагающей системой для этой модели является Bigtable[@changBigtableDistributedStorage2008]. В ней данные хранятся в виде наборов столбцов одинакового типа. При этом таких наборов может быть максимум сотни, в отличие от реляционной модели в которой может быть неограниченное количество столбцов. Дальше идет _графовая_ модель. Она строится на основе модели графа из теории графов. Ее преимущество состоит в том что она позволяет эффективно обрабатывать данные с большим числом связей между объектами разной природы. 

Следующий этап это поиск упоминаний в самом тексте страницы. В отличие от поисковых пауков он проходит весь текст содержащийся на ней, пытаясь найти нужную последовательность символов. Это можно делать как каждый раз проходя один и тот же текст или же строить инвертированный индекс[@zobelInvertedFilesText2006]. Он представляет собой структуру данных, в которой каждому слову сопоставляются места, где оно упоминается в различных текстах. Прежде чем строить подобный индекс, слова нужно привести в некоторую единую форму. Одним из таких решений является стемминг. Этот процесс оставляет от слова только его основу, не обязательно являющуюся корнем этого слова. Одной из его наиболее популярных реализаций является стеммер Портера[@willettPorterStemmingAlgorithm2006], который в последствии перерос в отдельный проект Snowball[@SnowballLanguageStemming]. В общем виде все алгоритмы стемминга можно разбить на 3 группы [@jivaniComparativeStudyStemming2011]: усеченные, статистические и смешанные. Примером усеченного является алгоритм Портера. Статистических - концепция n-gram. Идея ее  заключается в том, что похожие слова имеют очень высокий процент совпадения. Алгоритм разбивает исходное слово на части длины n. Так для слова "слон" 3-граммами будут последовательности: "--с", "-сл", "сло", "лон", "он-", "н--", где "-" означает пробел. Достоинством этой модели является ее независимость от языка, а недостатком долгая работа и большие затраты памяти. При помощи статистических методов оценивается есть ли заданное слово в том или ином документе. Другим вариантом нормализации слов является лемматизация. Она приводит слова в начальную форму (учил -> учить, домов -> дом). Такой вариант позволяет сохранить больше контекста при поиске определенный фраз. Однако есть более высокая вероятность ошибиться и в итоге только усложнить поиск[@balakrishnanStemmingLemmatizationComparison2014]. Смешанная модель стемминга же сочетает в себе достоинства первых двух. Построенный при помощи этих методов инвертированный индекс позволяет значительно ускорить работу поисковых систем.

Дальше подходящие страницы следует отсортировать от наиболее вероятного к наименее (релевантность) на основе более продвинутых методов. Базово это можно выполнять при помощи различных вариаций алгоритма TF-IDF [@saltonInformationProcessingManagement1988], [@ramosUsingTFIDFDetermine]. Основная идея его заключается в том, что можно разбить формулу определения релевантности на 2 множителя. TF (term frequency) описывает как часто искомое слово встречается в тексте страницы (документе). Например $\frac{w}{W}$, где $w$ - количество совпадающих слов, $W$ - число всех слов в документе. IDF (inverse document frequency) же является величиной пропорциональной отношению релевантных документов ко всем документам коллекции. В качестве нее можно брать например $log(\frac{N}{n})$, где n - число релевантных документов, а N - число всех документов. Итоговая формула для определения релевантности получается перемножением TF$\times$IDF, что в примере равно $\frac{w}{W} log(\frac{N}{n})$.

Перейдем к самой задаче данной работы. Нужно научиться находить ученых в полученных текстах и определять связаны ли они. Под связностью двух людей (рецензента и рецензируемого) будем понимать наличие знакомства, дружбы, родства или других отношений, которые могут оказать влияние на вынесение рецензентом вердикта на научную работу рецензируемого. Одним из вариантов которыми это можно сделать являются графы знаний[@hoganKnowledgeGraphs2022]. 

Дать определение онтологий, наиболее известных графов знаний, как они строятся, определяются, кто они такие и с чем их едят. Еще рассказть про word2vec и измерение близости текстов при помощи косинусного расстояния. Если придумаю дополнить про связность.


